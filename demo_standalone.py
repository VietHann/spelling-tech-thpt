# demo_standalone.py
# -*- coding: utf-8 -*-
"""
Standalone demo for Advanced Corrector (kh√¥ng c·∫ßn ch·∫°y server)
"""

import os
import sys
from advanced_corrector import VietnamesePreprocessor, MultiDetector, AdvancedCorrector

def print_section(title):
    """Print section header"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)

def demo_preprocessor():
    """Demo preprocessing"""
    print_section("DEMO 1: PREPROCESSING")
    
    preprocessor = VietnamesePreprocessor(use_word_segmenter=False)
    
    test_text = "Xin ch√†o! Email: test@example.com v√† website: https://example.com"
    
    print(f"\nüìù Input text:")
    print(f"   {test_text}")
    
    result = preprocessor.preprocess(test_text, protect_patterns=True)
    
    print(f"\n‚úÖ Preprocessing results:")
    print(f"   Normalized:  {result['normalized']}")
    print(f"   Sentences:   {result['sentences']}")
    print(f"   Tokens:      {result['tokens']}")
    
    if result['protected_map']:
        print(f"\nüîí Protected patterns:")
        for placeholder, original in result['protected_map'].items():
            print(f"   {placeholder} ‚Üí {original}")

def demo_detector():
    """Demo multi-detector"""
    print_section("DEMO 2: MULTI-DETECTOR")
    
    # Check if models exist
    det_dir = os.environ.get("DET_DIR", "outputs/detector")
    lexicon_path = "data/vi_lexicon.txt"
    
    if not os.path.exists(det_dir):
        print(f"\n‚ö†Ô∏è  Detector model not found at: {det_dir}")
        print(f"   Train detector first:")
        print(f"   python vi_spell_pipeline_plus.py --do_train_detector")
        return
    
    if not os.path.exists(lexicon_path):
        print(f"\n‚ö†Ô∏è  Lexicon not found. Creating sample lexicon...")
        os.system("python prepare_data.py --create_sample")
    
    print(f"\nüì¶ Loading detector...")
    print(f"   Model: {det_dir}")
    print(f"   Lexicon: {lexicon_path}")
    
    detector = MultiDetector(
        token_classifier_dir=det_dir,
        lexicon_path=lexicon_path,
        weight_oov=0.4,
        weight_mlm=0.0,  # Disabled (slow)
        weight_classifier=0.6,
    )
    
    print(f"   ‚úì Loaded (lexicon size: {len(detector.lexicon)})")
    
    # Test cases
    test_cases = [
        {
            'text': "T√¥i ƒëang h·ªçc ti·∫øng Vi·ªát",
            'expected': "No errors (correct sentence)"
        },
        {
            'text': "T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát",
            'expected': "Errors: t√¥ii, ƒëangg, h·ªçcc"
        },
        {
            'text': "Xin ch√†o b·∫°n nh√©",
            'expected': "No errors"
        },
    ]
    
    for i, case in enumerate(test_cases, 1):
        text = case['text']
        expected = case['expected']
        
        print(f"\nüìù Test case {i}:")
        print(f"   Input:    {text}")
        print(f"   Expected: {expected}")
        
        # Tokenize
        from advanced_corrector import VietnamesePreprocessor
        preprocessor = VietnamesePreprocessor()
        tokens = preprocessor.split_syllables(text)
        
        # Detect
        detections = detector.detect(tokens, threshold=0.5, use_mlm=False)
        
        if detections:
            print(f"\n   ‚úì Detected {len(detections)} error(s):")
            for det in detections:
                print(f"     ‚Ä¢ Position {det.position}: '{det.token}' (confidence={det.confidence:.2f})")
                print(f"       Scores: OOV={det.detector_scores['oov']:.2f}, "
                      f"Classifier={det.detector_scores['classifier']:.2f}")
        else:
            print(f"\n   ‚úì No errors detected")

def demo_full_pipeline():
    """Demo full advanced corrector"""
    print_section("DEMO 3: FULL PIPELINE")
    
    det_dir = os.environ.get("DET_DIR", "outputs/detector")
    lexicon_path = "data/vi_lexicon.txt"
    
    if not os.path.exists(det_dir):
        print(f"\n‚ö†Ô∏è  Detector model not found. Skipping demo.")
        return
    
    if not os.path.exists(lexicon_path):
        print(f"\n‚ö†Ô∏è  Creating sample lexicon...")
        os.system("python prepare_data.py --create_sample")
    
    print(f"\nüì¶ Initializing Advanced Corrector...")
    
    corrector = AdvancedCorrector(
        detector_dir=det_dir,
        lexicon_path=lexicon_path,
        use_word_segmenter=False,
    )
    
    print(f"   ‚úì Ready")
    
    test_cases = [
        "T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát.",
        "Email: test@example.com v√† website: https://example.com",
        "H√¥m nay tr·ªùi ƒë·∫πpp qu√°!",
    ]
    
    for i, text in enumerate(test_cases, 1):
        print(f"\nüìù Test case {i}:")
        print(f"   Input: {text}")
        
        result = corrector.correct(
            text,
            detection_threshold=0.5,
            protect_patterns=True,
        )
        
        print(f"\n   Preprocessing:")
        print(f"     Tokens: {result['preprocessed']['tokens']}")
        
        if result['detections']:
            print(f"\n   Detection ({len(result['detections'])} error(s)):")
            for det in result['detections']:
                print(f"     ‚Ä¢ Position {det['position']}: '{det['token']}' (conf={det['confidence']:.2f})")
                scores = det['detector_scores']
                print(f"       OOV={scores['oov']:.2f}, MLM={scores['mlm']:.2f}, "
                      f"Classifier={scores['classifier']:.2f}")
        else:
            print(f"\n   Detection: No errors")
        
        if result['corrections']:
            print(f"\n   Corrections:")
            for corr in result['corrections']:
                print(f"     ‚Ä¢ Position {corr['position']}: '{corr['original']}' ‚Üí '{corr['correction']}'")
        else:
            print(f"\n   Corrections: None (Generator not implemented yet - Phase 2)")
        
        print(f"\n   Final: {result['final']}")

def main():
    """Main demo"""
    print("\n" + "=" * 70)
    print("  ADVANCED VIETNAMESE SPELL CORRECTOR - STANDALONE DEMO")
    print("=" * 70)
    print("\n  Phase 1: Preprocessing + Multi-Detector")
    print("  Note: Candidate Generation & Ranking will be in Phase 2")
    
    # Demo 1: Preprocessor (always works)
    demo_preprocessor()
    
    # Demo 2: Detector (requires model)
    demo_detector()
    
    # Demo 3: Full pipeline (requires model)
    demo_full_pipeline()
    
    # Summary
    print_section("SUMMARY")
    print("\n‚úÖ Phase 1 Features Demonstrated:")
    print("   ‚Ä¢ Unicode normalization (NFC)")
    print("   ‚Ä¢ Sentence splitting")
    print("   ‚Ä¢ Word segmentation (syllable-based)")
    print("   ‚Ä¢ Pattern protection (URLs, emails)")
    print("   ‚Ä¢ Multi-detector ensemble (OOV + Classifier)")
    print("   ‚Ä¢ Detailed detection scores")
    
    print("\nüöß Phase 2 (Coming Soon):")
    print("   ‚Ä¢ Candidate Generation (SymSpell, Telex/VNI, keyboard)")
    print("   ‚Ä¢ Noisy-Channel Ranking (LM + P_err + freq)")
    print("   ‚Ä¢ KenLM 5-gram integration")
    
    print("\nüìö Next Steps:")
    print("   1. Read documentation: ADVANCED_CORRECTOR_README.md")
    print("   2. Read summary: PHASE1_SUMMARY.md")
    print("   3. Start API server: uvicorn app:app --port 8000")
    print("   4. Test API: curl -X POST http://localhost:8000/correct_v2 \\")
    print("                     -H 'Content-Type: application/json' \\")
    print("                     -d '{\"text\": \"T√¥ii ƒëangg h·ªçcc\"}'")
    
    print("\n" + "=" * 70)

if __name__ == "__main__":
    main()

