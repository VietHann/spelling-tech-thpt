# Advanced Vietnamese Spell Corrector

## üìã T·ªïng quan

H·ªá th·ªëng s·ª≠a l·ªói ch√≠nh t·∫£ ti·∫øng Vi·ªát n√¢ng cao v·ªõi ki·∫øn tr√∫c 3 t·∫ßng:

### **TIER 1: Realtime (ƒê√£ tri·ªÉn khai Phase 1)**
- ‚úÖ **Preprocessing**: Unicode NFC, sentence split, word segmentation
- ‚úÖ **Multi-Detector**: OOV + masked-LM NLL spike + token-classifier ensemble
- üöß **Candidate Generator**: SymSpell + Telex/VNI + keyboard adjacency (Phase 2)
- üöß **Noisy-Channel Ranker**: Feature stack v·ªõi LM + P_err + freq (Phase 2)
- üöß **Global Search**: Viterbi beam search (Phase 3)

### **TIER 2: Heavy / Fix All (Phase 3)**
- üöß **GEC Seq2Seq**: T5/BART v·ªõi constrained decoding
- üöß **Global Rescoring**: External LM + penalty

### **TIER 3: Post-processing & UX (Phase 3)**
- üöß **Post-rules**: Punctuation, whitespace, capitalization
- üöß **UX**: Underline errors, suggestions, explain

---

## üöÄ C√†i ƒë·∫∑t

### 1. Dependencies

```bash
# Core dependencies (ƒë√£ c√≥)
pip install torch transformers fastapi uvicorn

# Optional: Word segmentation (khuy·∫øn ngh·ªã)
pip install underthesea
# ho·∫∑c
pip install pyvi

# Optional: Testing
pip install requests
```

### 2. Chu·∫©n b·ªã d·ªØ li·ªáu

#### T·∫°o lexicon m·∫´u (cho testing):
```bash
python prepare_data.py --create_sample
```

T·∫°o file `data/vi_lexicon.txt` v·ªõi ~200 t·ª´ ph·ªï bi·∫øn.

#### T·∫°o lexicon t·ª´ Hunspell (production):
```bash
# Download Hunspell Vietnamese dictionary
wget https://raw.githubusercontent.com/LibreOffice/dictionaries/master/vi/vi_VN.dic

# Build lexicon
python prepare_data.py --hunspell_dic vi_VN.dic --output_dir data
```

#### T·∫°o lexicon t·ª´ corpus:
```bash
# N·∫øu b·∫°n c√≥ corpus ti·∫øng Vi·ªát (plain text)
python prepare_data.py --corpus path/to/corpus.txt --output_dir data
```

### 3. Train models (n·∫øu ch∆∞a c√≥)

```bash
# Train detector
python vi_spell_pipeline_plus.py --do_train_detector \
    --det_out outputs/detector \
    --det_epochs 3

# Train corrector
python vi_spell_pipeline_plus.py --do_train_corrector \
    --corr_out outputs/corrector \
    --corr_epochs 3
```

---

## üìñ S·ª≠ d·ª•ng

### 1. Ch·∫°y API server

```bash
# Set environment variables (optional)
export DET_DIR=outputs/detector
export CORR_DIR=outputs/corrector
export LEXICON_PATH=data/vi_lexicon.txt

# Start server
uvicorn app:app --host 0.0.0.0 --port 8000
```

### 2. Test endpoints

#### Health check:
```bash
curl http://localhost:8000/health
```

Response:
```json
{
  "status": "ok",
  "device": "cuda",
  "det_dir": "outputs/detector",
  "corr_dir": "outputs/corrector",
  "advanced_corrector": {
    "enabled": true,
    "has_lexicon": true,
    "lexicon_size": 234
  }
}
```

#### Original corrector (v1):
```bash
curl -X POST http://localhost:8000/correct \
  -H "Content-Type: application/json" \
  -d '{"text": "T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát"}'
```

#### Advanced corrector (v2):
```bash
curl -X POST http://localhost:8000/correct_v2 \
  -H "Content-Type: application/json" \
  -d '{
    "text": "T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát",
    "detection_threshold": 0.5,
    "use_oov": true,
    "use_mlm": false,
    "use_classifier": true,
    "protect_patterns": true
  }'
```

Response:
```json
{
  "input": "T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát",
  "preprocessed": {
    "normalized": "T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát",
    "tokens": ["T√¥ii", "ƒëangg", "h·ªçcc", "ti·∫øng", "Vi·ªát"],
    "sentences": ["T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát"]
  },
  "detections": [
    {
      "position": 0,
      "token": "T√¥ii",
      "confidence": 0.85,
      "detector_scores": {
        "oov": 1.0,
        "mlm": 0.0,
        "classifier": 0.7
      }
    }
  ],
  "corrections": [],
  "final": "T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát"
}
```

### 3. Python API

```python
from advanced_corrector import AdvancedCorrector

# Initialize
corrector = AdvancedCorrector(
    detector_dir="outputs/detector",
    lexicon_path="data/vi_lexicon.txt",
    use_word_segmenter=False,  # Set True if underthesea installed
)

# Correct text
result = corrector.correct(
    text="T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát",
    detection_threshold=0.5,
    protect_patterns=True,
)

print(f"Input: {result['input']}")
print(f"Detections: {len(result['detections'])}")
print(f"Final: {result['final']}")
```

### 4. Run tests

```bash
python test_advanced_corrector.py
```

---

## üîß C·∫•u h√¨nh

### Multi-Detector weights

Trong `app.py` ho·∫∑c khi kh·ªüi t·∫°o `MultiDetector`:

```python
detector = MultiDetector(
    token_classifier_dir="outputs/detector",
    lexicon_path="data/vi_lexicon.txt",
    # Ensemble weights (should sum to 1.0)
    weight_oov=0.3,          # OOV detection
    weight_mlm=0.3,          # Masked-LM spike (slow!)
    weight_classifier=0.4,   # Token classifier
)
```

**L∆∞u √Ω**: 
- `use_mlm=True` r·∫•t ch·∫≠m (masked-LM inference cho m·ªói token)
- Khuy·∫øn ngh·ªã: `use_mlm=False` cho realtime, `use_mlm=True` cho "Fix All"

### API parameters

#### `/correct_v2` endpoint:

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `text` | string | required | Text to correct |
| `detection_threshold` | float | 0.5 | Confidence threshold [0, 1] |
| `protect_patterns` | bool | true | Protect URLs, emails, code |
| `use_oov` | bool | true | Enable OOV detector |
| `use_mlm` | bool | false | Enable masked-LM detector (slow!) |
| `use_classifier` | bool | true | Enable token classifier |

---

## üìä So s√°nh v1 vs v2

| Feature | v1 (Original) | v2 (Advanced) |
|---------|---------------|---------------|
| **Preprocessing** | Simple syllable split | Unicode NFC + sentence split + word segment |
| **Detection** | Token classifier only | Multi-detector ensemble (OOV + MLM + classifier) |
| **Pattern protection** | ‚ùå | ‚úÖ URLs, emails, code |
| **Detailed scores** | ‚ùå | ‚úÖ Per-detector confidence |
| **Speed** | Fast | Fast (if MLM disabled) |
| **Accuracy** | Good | Better (with lexicon) |

---

## üõ£Ô∏è Roadmap

### ‚úÖ Phase 1 (Completed)
- [x] Preprocessing pipeline
- [x] Multi-detector (OOV + MLM + classifier)
- [x] API endpoint `/correct_v2`
- [x] Test suite
- [x] Documentation

### üöß Phase 2 (Next)
- [ ] Candidate Generator
  - [ ] SymSpell (with/without diacritics)
  - [ ] Telex/VNI conversion
  - [ ] Keyboard adjacency
  - [ ] Split/Join
- [ ] Noisy-Channel Ranker
  - [ ] KenLM 5-gram
  - [ ] Feature stack (LM + P_err + freq + edit + ortho)
- [ ] Data preparation
  - [ ] Train KenLM from corpus
  - [ ] Build frequency dict
  - [ ] Toneless-to-toned mapping

### üöß Phase 3 (Future)
- [ ] Global Search (Viterbi beam)
- [ ] GEC Seq2Seq (T5/BART)
- [ ] Global Rescoring
- [ ] Post-processing rules
- [ ] UX features (underline, explain, dictionary)

---

## üêõ Troubleshooting

### 1. "Detector model not found"
```bash
# Train detector first
python vi_spell_pipeline_plus.py --do_train_detector
```

### 2. "Lexicon not found"
```bash
# Create sample lexicon
python prepare_data.py --create_sample
```

### 3. "MLM detection is slow"
```bash
# Disable MLM in API request
curl -X POST http://localhost:8000/correct_v2 \
  -d '{"text": "...", "use_mlm": false}'
```

### 4. "underthesea not found"
```bash
# Install word segmenter (optional)
pip install underthesea

# Or use simple syllable splitting (default)
# Set use_word_segmenter=False in AdvancedCorrector
```

---

## üìù Examples

### Example 1: Basic correction
```python
corrector.correct("T√¥ii ƒëangg h·ªçcc ti·∫øng Vi·ªát")
# Detects: T√¥ii, ƒëangg, h·ªçcc (OOV + classifier)
```

### Example 2: Pattern protection
```python
corrector.correct(
    "Email: test@example.com v√† website: https://example.com",
    protect_patterns=True
)
# Protects: test@example.com, https://example.com
```

### Example 3: Custom threshold
```python
corrector.correct(
    "H√¥m nay tr·ªùi ƒë·∫πpp qu√°",
    detection_threshold=0.7  # Higher threshold = fewer detections
)
```

---

## üìö References

- **Hunspell Vietnamese**: https://github.com/LibreOffice/dictionaries/tree/master/vi
- **underthesea**: https://github.com/undertheseanlp/underthesea
- **pyvi**: https://github.com/trungtv/pyvi
- **SymSpell**: https://github.com/wolfgarbe/SymSpell
- **KenLM**: https://github.com/kpu/kenlm

